<!DOCTYPE html>
<html>
<head>
    <title>Projects</title>
<style>
div.container {
    width: 100%;
    border: 1px solid gray;
}

header, footer {
    padding: 1em;
    color: white;
    background-color: #224870;
    clear: left;
    text-align: center;
}

article {
    margin-left: auto;
    margin-right: auto;
    padding: 1em;
    overflow: hidden;
    align: center;
}
ul {
    list-style-type: decimal;
}
</style>
</head>
<body>

<div class="container">

<header>
   <h1>Projects</h1>
</header>


<article id="idx" align="left">
<h3>Projects</h3>
  <ul>
    <li><a href="#pynn">Neural Network Design</a></li>
    <li><a href="#slam">SLAM</a></li>
    <li><a href="#sfm">Tracking</a></li>
    <li><a href="#qlearn">Q-Learning: PACMAN</a></li>
    <li><a href="#qlearngrid">Q-Learning: Gridworld</a></li>
    <li><a href="#kmeans">KMeans Image Segmentation</a></li>
    <li><a href="#facewarp">Face Warping</a></li>
    <li><a href="#facerep">Face Replacement</a></li>
    <li><a href="#gestcont">Gesture Controlled Robotic Arm</a></li>
    <li><a href="#3dfruit">Fruit Tree Reconstruction</a></li>
  </ul>
  <a href="index.html"> << Back</a>
</article>


<article id="pynn">
<h3>Neural Network Classification of Handwritten Digits using Python</h3>
    <p align="justify">
I built an artificial neural network from scratch to classify hand written digits using Back-propogation. There are 64 hidden units in the hidden layer, which are each represented by a 20x20 pixel grid cell. To generate this video, I lowered my learning rate and trained the neural network over ~550 epochs (for better visualisation over epochs), where each frame in the video represents the hidden layer at a given epoch.<br><br>

Each hidden unit represents different stroke detections and patterns learnt from the handwritten digits. This was easily one of the most exciting and rewarding projects I've worked on.<br><br>

I initially built this neural network for an assignment for the CIS519 "Introduction to Machine Learning" course taught by Eric Eaton at the University of Pennsylvania. This program was written in Python, using only the 'Image' library for image representation of the hidden units.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/PuHK6tGzYMA" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>

<article id="slam">
<h3>SLAM Robot Mapping and Localization in 2D</h3>
    <p align="justify">
White pixels indicate obstacles and boundaries. Varying levels of black to grey pixels represent traversable pathways. The blue pixels indicate current Lidar sensor hits and the yellow triangle represents the robot with the longer edge pointing in the direction of the robots current orientation.<br><br>

This project was part of the ESE650 : Learning in Robotics course taught by Dr. Daniel Lee at The University of Pennsylvania.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/ldSHBoWo6Rc" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>

<article id="sfm">
<h3>Structure From motion</h3>
  <p align="justify">
Part 1: Here are some of the initial steps in the SFM pipeline, which are a result of estimating the Fundamental Matrix between two images in a sequence and performing RANSAC to get rid of erroneous outliers and get a set of at least 8 strong correspondences. The next step involves estimating the Essential Matrix and generating a set of 4 possible camera pose configurations. The video illustrates the RANSAC algorithm working with different threshold values to refine the strength of the correspondences and also a visualisation of Epipolar Lines is shown.<br><br>

Part 2: This is the second half of the SFM project that I just completed for the CIS580 Machine Perception course taught by Dr. Jainbo Shi. In this video I visualise results from the final bundle adjustment step (the final point cloud) followed by the previous Perspective n Point algorithm's results. Here the different colored points denote the points added from different frames in the image sequence and the camera positions are shown in red. The next visualisation is the previous triangulation step where two frames from the image sequence are used to generate a set of triangulated 3D points. The triangulated points in black indicate non-linear refinement.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/qGq7OZsAGTk" frameborder="0" allowfullscreen></iframe><br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/ToxJx3tqmDU" frameborder="0" allowfullscreen></iframe><br><br>
</p>
</article>


<article id="qlearn">
<h3>Approimate Q-Learning on PACMAN</h3>
<p align="justify">
This was one of my results for the 'Reinforcement Learning' project/homework that was part of the CIS519 'Introduction to Machine Learning' course taught by Eric Eaton at the University of Pennsylvania.<br><br>

This Q-Learning agent uses function approximation and state abstraction to help PACMAN win as many games as possible. Here, the agent is trained only on 50 games and is exposed to 10 test game instances. In the above video, PACMAN manages to win 8 out of 10 times, with a 80% accuracy. The agent learns weights for state features where different states could share the same features. There is a feature function fn(state,action) over different pairs which is a vector of feature vectors. <br><br>

This project was implemented in Python using many support methods and programs to replicate the pacman and gridworld experience along with a python library to implement Markov Decision Processes and other libraries like util.py which helped build the q-Learning agent.<br><br>

Video has been rendered at 6x the actual speed.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/8mHMUP-b_C8" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>


<article id="qlearngrid">
<h3>Q-Learning on GRIDWORLD</h3>
<p align="justify">
The next part of the 'Reinforcement Learning' project was to implement a Q-Learning Agent given the gridworld, Markov Decision Process etc. framework provided. Unlike the Value Iteration agent, the Q Learning agent actually 'learns' from experience as is seen in the video. Epsilon-greedy action selection is used which chooses random actions epsilon of the time and follows the best Q-Value during all other instances.<br><br>

This was one of my results for the CIS519 'Introduction to Machine Learning' taught by Eric Eaton at the University of Pennsylvania.<br><br>

Video is rendered at 4x of the actual speed.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/RTu7G0y4Os4" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>


<article id="kmeans">
<h3>Image Segmentation with K-Means Clustering using Python</h3>
<p align="justify">
This was part of an assignment for the CIS519: Introduction to Machine Learning course at Penn, taught by Eric Eaton. The video shows my K-Means Clustering algorithm running on an image, iterating from K=1 to K=80 clusters, with the last 3 frames being the original image. I wrote this program in python, using the PIL library for image representation.<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/qstdP1DSUKQ" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>

<article id="facewarp">
<h3>Using Thin Plate Splines for Face Warping</h3>
<p align="justify">
This was part of a project for the CIS580 : Computer Vision and Computational Photography at Penn taught by Prof. Jianbo Shi. I referred to the lecture notes as well as <i>"Principal Warps: Thin Plate Splines and Decompositions of Deformations" by Fred. L Bookstein.</i><br><br>Photograph Source : <a href="https://commons.wikimedia.org/wiki/File:Official_portrait_of_Barack_Obama.jpg" target="_blank">Image</a><br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/4ZrFsuFPKlk" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>

<article id="facerep">
<h3>Face Replacement</h3>
<p align="justify">
This video was one of the test videos for a face replacement project that I was a part of in the CIS581 course at the University of Pennsylvania.<br><br>
Original video: <a href="https://www.youtube.com/watch?v=_sQSXwdtxlY" target="_blank">YouTube</a><br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/lD6QKSdWJpA" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>


<article id="gestcont">
<h3>Gesture Controlled Robot Arm</h3>
<p align="justify">
Here's a video of our Gesture Controlled Robotic Arm project. We're using a Leap Motion device on the client side along with our custom designed web application that communicates wirelessly with a 5-DOF Robotic Arm on the server side. The Robotic Arm was made using 5 BMS 860DMax servos, a laser cut plastic frame and an Arduino Yun microcontroller.<br><br>
Link to code repository: <a href="https://github.com/ShreyasSkandan/robotic-arm" target="_blank"> GitHub </a><br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/kUqru2KU-xE" frameborder="0" allowfullscreen></iframe><br><br></p>
</article>


<article id="3dfruit">
<h3>3D Reconstruction of Fruit Tree</h3>
<p align="justify">
I collected a dataset of an apple tree while I was at Biglerville,PA using my cellphone (Nexus 6P) and tested out some sparse 3D reconstruction using VisualSFM and some additional methods to clean up the point cloud and plot my trajectory<br><br>
<iframe width="415" height="215" src="https://www.youtube.com/embed/YpgmP8t5TPg" frameborder="0" allowfullscreen></iframe><br><br></p>
</p>
</article>



<footer>Shreyas Skandan</footer>

</div>

</body>
</html>