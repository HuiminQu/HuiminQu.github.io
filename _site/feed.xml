<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-09-30T03:54:06-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Huimin Qu</title><subtitle>Undergraduate Student at Xiamen University</subtitle><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><entry><title type="html">DFuseNet: Deep Fusion of RGB and Sparse Depth Information</title><link href="http://localhost:4000/posts/dfusenet/" rel="alternate" type="text/html" title="DFuseNet: Deep Fusion of RGB and Sparse Depth Information" /><published>2019-04-18T00:00:00-07:00</published><updated>2019-04-18T00:00:00-07:00</updated><id>http://localhost:4000/posts/dfusenet</id><content type="html" xml:base="http://localhost:4000/posts/dfusenet/">&lt;p&gt;Code for our paper &lt;em&gt;“DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion”&lt;/em&gt; is now on GitHub -
&lt;a href=&quot;https://github.com/ShreyasSkandanS/DFuseNet&quot;&gt;CODE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The ARXIV paper can be found &lt;a href=&quot;https://arxiv.org/pdf/1902.00761.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dfusenet_kitti.png&quot; alt=&quot;DFuseNetKitti&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also present a small dataset of calibrated RGB and LiDAR data from a short drive around Philadelphia as an additional test set. This dataset can be found &lt;a href=&quot;https://github.com/ShreyasSkandanS/DFuseNet&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;
&lt;p&gt;In this paper we propose a convolutional neural network that is designed to upsample a series of sparse range measurements based on the contextual cues gleaned from a high resolution intensity image. Our approach draws inspiration from related work on super-resolution and in-painting. We propose a novel architecture that seeks to pull contextual cues separately from the intensity image and the depth features and then fuse them later in the network. We argue that this approach effectively exploits the relationship between the two modalities and produces accurate results while respecting salient image structures. We present experimental results to demonstrate that our approach is comparable with state of the art methods and generalizes well across multiple datasets.&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="nvidia" /><summary type="html">Code for our paper “DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion” is now on GitHub - CODE</summary></entry><entry><title type="html">DARPA Subterranean Challenge Integrated Exercise</title><link href="http://localhost:4000/posts/darpastix/" rel="alternate" type="text/html" title="DARPA Subterranean Challenge Integrated Exercise" /><published>2019-04-07T00:00:00-07:00</published><updated>2019-04-07T00:00:00-07:00</updated><id>http://localhost:4000/posts/darpastix</id><content type="html" xml:base="http://localhost:4000/posts/darpastix/">&lt;p&gt;Our team (PLUTO) just got back from a successful run at the DARPA STIX in Colorado.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/stixteam.jpg&quot; alt=&quot;TEAM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We took platforms from both Ghost Robotics and Exyn Technologies:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ghost Robotics Vision Platform:&lt;/strong&gt;
&lt;img src=&quot;/images/ghost_1.png&quot; alt=&quot;GhostRobotics1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ghost Robotics Vision Platform:&lt;/strong&gt;
&lt;img src=&quot;/images/ghost_2.png&quot; alt=&quot;GhostRobotics2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ghost Robotics Vision Platform and Exyn Aerial Platform:&lt;/strong&gt;
&lt;img src=&quot;/images/stix_platforms.png&quot; alt=&quot;RobotTeam&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To keep track of our team and our progress through this challenge, watch our &lt;a href=&quot;https://pluto-subt.github.io/index.html&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="subterraneanchallenge" /><category term="robotics" /><category term="computer vision" /><category term="darpa" /><summary type="html">Our team (PLUTO) just got back from a successful run at the DARPA STIX in Colorado.</summary></entry><entry><title type="html">ICRA 2019 Accepted Papers</title><link href="http://localhost:4000/posts/icra2019papers/" rel="alternate" type="text/html" title="ICRA 2019 Accepted Papers" /><published>2019-02-21T00:00:00-07:00</published><updated>2019-02-21T00:00:00-07:00</updated><id>http://localhost:4000/posts/icra_accepted</id><content type="html" xml:base="http://localhost:4000/posts/icra2019papers/">&lt;p&gt;Two papers that I was a part of:&lt;/p&gt;

&lt;h2 id=&quot;1-the-open-vision-computer-an-integrated-sensing-and-compute-system-for-mobile-robots&quot;&gt;1. The Open Vision Computer: An Integrated Sensing and Compute System for Mobile Robots&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.07674&quot;&gt;Paper (arXiv)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;abstract&quot;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;In this paper we describe the Open Vision Computer (OVC) which was designed to support high speed, vision guided autonomous drone flight. In particular our aim was to develop a system that would be suitable for relatively small-scale flying platforms where size, weight, power consumption and computational performance were all important considerations. This manuscript describes the primary features of our OVC system and explains how they are used to support fully autonomous indoor and outdoor exploration and navigation operations on our Falcon 250 quadrotor platform.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/watch?v=dMxgNf8cXkI&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/dMxgNf8cXkI/0.jpg&quot; alt=&quot;OVC&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-real-time-dense-depth-estimation-by-fusing-stereo-with-sparse-depth-measurements&quot;&gt;2. Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.07677&quot;&gt;Paper (arXiv)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;abstract-1&quot;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present an approach to depth estimation that fuses information from a stereo pair with sparse range measurements derived from a LIDAR sensor or a range camera. The goal of this work is to exploit the complementary strengths of the two sensor modalities, the accurate but sparse range measurements and the ambiguous but dense stereo information. These two sources are effectively and efficiently fused by combining ideas from anisotropic diffusion and semi-global matching.&lt;/p&gt;

&lt;p&gt;We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets, using randomly sampled ground truth range measurements as our sparse depth input. We achieve significant performance improvements with a small fraction of range measurements on both datasets. We also provide qualitative results from our platform using the PMDTec Monstar sensor. Our entire pipeline runs on an NVIDIA TX-2 platform at 5Hz on 1280x1024 stereo images with 128 disparity levels.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/watch?v=p_jCRGMqE7Y&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/p_jCRGMqE7Y/0.jpg&quot; alt=&quot;StereoDepth&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have been accepted to the “IEEE International Conference on Robotics and Automation 2019” (ICRA 2019). I will be attending to present this work, so feel free to reach out to me if you’re there.&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="nvidia" /><summary type="html">Two papers that I was a part of:</summary></entry><entry><title type="html">Stereo and Sparse Depth Fusion</title><link href="http://localhost:4000/posts/stereofusion/" rel="alternate" type="text/html" title="Stereo and Sparse Depth Fusion" /><published>2018-11-21T00:00:00-07:00</published><updated>2018-11-21T00:00:00-07:00</updated><id>http://localhost:4000/posts/stereofusion</id><content type="html" xml:base="http://localhost:4000/posts/stereofusion/">&lt;p&gt;Code for our paper &lt;em&gt;“Real Time Dense Depth Estimation by Fusing Stereo with
Sparse Depth Measurements”&lt;/em&gt; is now on GitHub -
&lt;a href=&quot;https://github.com/ShreyasSkandanS/stereo_sparse_depth_fusion&quot;&gt;CODE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The ARXIV paper can be found &lt;a href=&quot;https://arxiv.org/pdf/1809.07677.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;
&lt;p&gt;We present an approach to depth estimation that fuses information from a stereo pair with sparse range measurements derived from a LIDAR sensor or a range camera. The goal of this work is to exploit the complementary strengths of the two sensor modalities, the accurate but sparse range measurements and the ambiguous but dense stereo information. These two sources are effectively and efficiently fused by combining ideas from anisotropic diffusion and semi-global matching.&lt;/p&gt;

&lt;p&gt;We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets, using randomly sampled ground truth range measurements as our sparse depth input. We achieve significant performance improvements with a small fraction of range measurements on both datasets. We also provide qualitative results from our platform using the PMDTec Monstar sensor. Our entire pipeline runs on an NVIDIA TX-2 platform at 5Hz on 1280x1024 stereo images with 128 disparity levels.&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="nvidia" /><summary type="html">Code for our paper “Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements” is now on GitHub - CODE</summary></entry><entry><title type="html">GTC DC 2018 - Jetson AGX Xavier Developer Day</title><link href="http://localhost:4000/posts/gtcdc-talk/" rel="alternate" type="text/html" title="GTC DC 2018 - Jetson AGX Xavier Developer Day" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://localhost:4000/posts/gtcdc</id><content type="html" xml:base="http://localhost:4000/posts/gtcdc-talk/">&lt;p&gt;I was invited to speak at the NVIDIA GPU Technology Conference for at their
NVIDIA Jetson AGX Xavier Developer Day. Here is a video from the talk. The talk
is mainly centered around the use of the NVIDIA Jetson platform on our
quadrotors and some information regarding the autonomous UAV software stack that was designed
at our lab.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FLunb5Y-USI&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/FLunb5Y-USI/0.jpg&quot; alt=&quot;GTCDC2018&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Speaker List:
&lt;img src=&quot;/images/gtcdc.png&quot; alt=&quot;gtcdc&quot; /&gt;&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="nvidia" /><summary type="html">I was invited to speak at the NVIDIA GPU Technology Conference for at their NVIDIA Jetson AGX Xavier Developer Day. Here is a video from the talk. The talk is mainly centered around the use of the NVIDIA Jetson platform on our quadrotors and some information regarding the autonomous UAV software stack that was designed at our lab.</summary></entry><entry><title type="html">Pytorch Scribbles</title><link href="http://localhost:4000/posts/pytorch_notes/" rel="alternate" type="text/html" title="Pytorch Scribbles" /><published>2018-10-06T00:00:00-07:00</published><updated>2018-10-06T00:00:00-07:00</updated><id>http://localhost:4000/posts/pytorchtips</id><content type="html" xml:base="http://localhost:4000/posts/pytorch_notes/">&lt;h1 id=&quot;pytorch-scribble-pad&quot;&gt;PyTorch Scribble Pad&lt;/h1&gt;

&lt;p&gt;This page is a collection of notes and tips for myself in getting familiar with
the workings of PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Transfering Weights&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you have a pretrained network A with some layers A:{x,y,z} and you have a new
network architecture with some layers B:{w,x,y,z,a}, and you wish to transfer
weights learned from network A for layers {x,y,z} to B, you can do it using the
following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pretrained_model_weights = torch.load('../path/model.pth')
new_model_weights = model.state_dict()
pretrained_model_weights = {k: v for k, v in pretrained_model_weights.items() if k in new_model_weights}
new_model_weights.update(pretrained_model_weights)
model.load_state_dict(new_model_weights)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. Transferring Weights and Distributed Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I use different shared machines with multiple GPUs and often use different GPU
ids on different days based on availability. I also occasionally switch between
multi-gpu training and single-gpu training etc. I noticed that the
&lt;strong&gt;nn.DataParallel&lt;/strong&gt; class can be a bit tricky to navigate for such usage
conditions, specially if you’re not aware of how models are saved to file, which
I wasn’t at the time.&lt;/p&gt;

&lt;p&gt;If you’re training a model on a multi-gpu setup and save the model naively, you
are unknowingly appending a “module” tag to the &lt;em&gt;state_dict&lt;/em&gt; elements present in
the model parameters key-value store, and it appears that this assumes some
implicit binding to specific GPUs (I could be wrong?). But if you naively try to
load and run this model on a different multi-gpu setup, you will notice an error
that says a specific tensor is meant to run on a specific GPU. We don’t want
that.&lt;/p&gt;

&lt;p&gt;What the error message looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RuntimeError: Expected tensor for argument #1 'input' to have the same device as
tensor for argument #2 'weight'; but device 0 does not equal 1 (while checking
arguments for cudnn_convolution)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The easiest suggested fix is to iterate through the model &lt;em&gt;state_dict&lt;/em&gt; key-value
store and remove the “module.” binding like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pretrained_model = checkpoint['model']
new_model = SomeNetwork()
from collections import OrderedDict
new_model_dict = OrderedDict()
for k,v in pretrained_model.state_dict().items():
    # Drop the &quot;Module.&quot; characters from the name
    name = k[7:]
    new_model_dict[name] = v
new_model.load_state_dict(new_model_dict)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/2&quot;&gt;Credit&lt;/a&gt;&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="deep learning" /><category term="pytorch" /><summary type="html">PyTorch Scribble Pad</summary></entry><entry><title type="html">Jetson Xavier - Initial Thoughts</title><link href="http://localhost:4000/posts/jetsonxavier-initialthoughts/" rel="alternate" type="text/html" title="Jetson Xavier - Initial Thoughts" /><published>2018-10-03T00:00:00-07:00</published><updated>2018-10-03T00:00:00-07:00</updated><id>http://localhost:4000/posts/jetsonxavier</id><content type="html" xml:base="http://localhost:4000/posts/jetsonxavier-initialthoughts/">&lt;p&gt;Ever since the Jetson Xavier was announced, I’ve been itching to get my hands on
one of them to put it through it’s paces. Thanks to James over at &lt;a href=&quot;https://www.ghostrobotics.io/&quot;&gt;Ghost
Robotics&lt;/a&gt; I finally get to play with one of
these. I’ve spent a fair amount of time with the Jetson TX1 and Jetson TX2 and I
will be making direct comparisons to the Xavier’s predecessor, the TX2.&lt;/p&gt;

&lt;h1 id=&quot;hardware-and-design&quot;&gt;Hardware and Design&lt;/h1&gt;

&lt;p&gt;Out of the box, the Xavier devkit in no way resembles the previous devkits, and
that’s a good thing because the previous dev kits had limited to no practical
value for what we use them for (mobile robots -
&lt;a href=&quot;https://osrf.github.io/ovc/assets/images/ovc1-drone.png&quot;&gt;Falcon 250&lt;/a&gt; +
&lt;a href=&quot;http://open.vision.computer&quot;&gt;Open Vision Computer&lt;/a&gt;).
The entire physical footprint of the devkit is slightly larger than the actual module, and it appears that it couldn’t
get much smaller even in a tightly packed carrier board (good job @nvidia). However, the first
reaction is to the weight of this unit. It weighs roughly &lt;strong&gt;660gms&lt;/strong&gt; out of the box,
without the power supply. Since this is a loaner unit and since I cannot gut the
thing yet, I will guesstimate that most of this weight is the extremely heavy
heat sink and casing. I will update this post once I get my own unit and take
all of that off! The unit is a bit tall too but it’s mostly 70% heatsink and fan
enclosure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: TX2 devkit vs Xavier devkit&lt;/strong&gt; (food truck cash card for size comparison)
&lt;img src=&quot;/images/IMG_3345.jpg&quot; alt=&quot;devkit-comparisons-1&quot; /&gt;
&lt;strong&gt;Figure 2: Height Comparison&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3346.jpg&quot; alt=&quot;devkit-comparisons-2&quot; /&gt;
&lt;strong&gt;Figure 3: The incredible bulk&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3344.jpg&quot; alt=&quot;xavier-weight&quot; /&gt;
&lt;strong&gt;Figure 4: Dimensions&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3352.jpg&quot; alt=&quot;xavier-height&quot; /&gt;
&lt;strong&gt;Figure 5: Dimensions&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3347.jpg&quot; alt=&quot;xavier-width&quot; /&gt;
&lt;strong&gt;Figure 6: Under the carrier hood&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3349.jpg&quot; alt=&quot;xavier-carrier&quot; /&gt;
&lt;strong&gt;Figure 7: Power suply&lt;/strong&gt;
&lt;img src=&quot;/images/IMG_3354.jpg&quot; alt=&quot;xavier-ps&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding to the good news, this product seems well build and extremely well protected.
If weight isn’t a problem, I would strap one of these onto a robot directly
without the hassle of manufacturing or buying a separate carrier board.&lt;/p&gt;

&lt;p&gt;I would have liked if there was at least another USB Type A port. The
eSATAp+USB3.0 TypeA port is cool but I think most robotics peripherals are still
on Type A and I would have preferred not to bring the battle of dongles into the
robotics world, but oh well. The kind folks at NVIDIA do ship the devkits with
USB-C to Type-A dongles and don’t charge you extra for it (take that @apple).
Apart from the USB-C, the rest of the I/O is similar to the TX2 dev-kits. There’s an
additional M2 which will definitely prove usefull. For those that care, the
power supply adapter is now a bit smaller too. Now, onto the fun stuff..&lt;/p&gt;

&lt;h1 id=&quot;specifications-and-performance&quot;&gt;Specifications and Performance&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;CUDA Compatibility Major/Minor version number: 7.2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multiprocessors: 8&lt;/strong&gt; (TX2 has 2)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CUDA Cores/Mp: 64&lt;/strong&gt; (TX2 has 128)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Total CUDA Cores: 512&lt;/strong&gt; (TX2 has 256)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global Memory: ~16GB&lt;/strong&gt; (TX2 has ~8GB)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GPU Max Frequency: 1500GHz&lt;/strong&gt; (TX2 has 1300GHz)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Memory Clock Rate: 1500MHz&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Memory Bus Width: 256-bit&lt;/strong&gt; (TX2 has 128-bit)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8: Device Query&lt;/strong&gt;
&lt;img src=&quot;/images/device_query.png&quot; alt=&quot;device-query&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The CUDA Cores to Multiprocessor ratio is interesting. I will post a more
detailed follow up with actual benchmarks on my code soon. I suspect the Xavier
will be able to better handle multiple CUDA streams and kernel launches because
of this, and that is exciting.&lt;/p&gt;

&lt;p&gt;In the CPU realm, the Xavier brings 8 ARMv8 Processor cores, which seem to perform significantly
better than the TX2, where the Denver cores didn’t really make significant
contributions to performance. The CPU max frequency is 2265Hz and I did a little
stress test to see how hot things could get.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ./jetson_clocks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is somewhat of a baseline for CPU and GPU temperatures. The device was
idling when these were recorded. These are not freshly booted temperatures.
Those are in the late 30 degres celsius range.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8: Before CPU stress test:&lt;/strong&gt;
&lt;img src=&quot;/images/baseline_perf_thermal.png&quot; alt=&quot;thermal-baseline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s stress it out:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stress --cpu 8 --io 6 --vm 6 --vm-bytes 2048M --timeout 600s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 9: Stress temperatures:&lt;/strong&gt;
&lt;img src=&quot;/images/stress_temp.png&quot; alt=&quot;thermal-cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CPU-bound processes seem to be handled fairly well. I ran the stress test for 10
minutes each a few times and temperatures stayed in the 50s.&lt;/p&gt;

&lt;p&gt;To add some fuel to the fire, I threw in a pretty intensive GPU-bound process to
the mix (and dialed back the CPU stress io and vm parameters to 2).&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./nbody_opengles -benchmark -fp64 -fullscreen -numbodies=1000000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 10: GPU Stress temperatures:&lt;/strong&gt;
&lt;img src=&quot;/images/cpugpumax.png&quot; alt=&quot;hot-hot-hot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Things got hot. Both CPU and GPU internal temperatures began to cross the 70
degree mark. Temperatures remained in the 70s and didn’t appear to increase much
even at 100% CPU and 100% GPU usage.&lt;/p&gt;

&lt;p&gt;While the devkit seems to be well cooled, I suspect the Xavier will not take
well to having it’s heatsink, fan and casing thrown away (as we bravely do with
the Falcon 250, but that is an experiment I still intend on performing).&lt;/p&gt;

&lt;h1 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h1&gt;

&lt;p&gt;30-10-02: I think this is a great step forward when compared to the TX2 devkits.
Performance out of the box is impressive. A proper benchmark on existing TX2
code is next on the to-do list along with a more comprehensive thermal analysis
experiment without the fan and heat sink.&lt;/p&gt;</content><author><name>Huimin Qu</name><email>quhuimin@stu.xmu.edu.cn</email></author><category term="embedded" /><category term="gpu" /><category term="robotics" /><category term="computer vision" /><category term="nvidia" /><summary type="html">Ever since the Jetson Xavier was announced, I’ve been itching to get my hands on one of them to put it through it’s paces. Thanks to James over at Ghost Robotics I finally get to play with one of these. I’ve spent a fair amount of time with the Jetson TX1 and Jetson TX2 and I will be making direct comparisons to the Xavier’s predecessor, the TX2.</summary></entry></feed>